{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2461ad4",
   "metadata": {},
   "source": [
    "# AÅŸama 2: Veri Ã–n Ä°ÅŸleme\n",
    "\n",
    "**Proje**: Ã‡elik Levha Hata Tespiti  \n",
    "**Veri Seti**: Ã‡elik Levha Hata Veri Seti (1.941 Ã¶rnek)  \n",
    "**AmaÃ§**: Makine Ã¶ÄŸrenimi iÃ§in veriyi temizlemek ve hazÄ±rlamak  \n",
    "**Tarih**: AralÄ±k 2024\n",
    "\n",
    "---\n",
    "\n",
    "## Genel BakÄ±ÅŸ\n",
    "\n",
    "Bu defter ÅŸunlarÄ± kapsar:\n",
    "1. Ham veriyi yÃ¼kleme\n",
    "2. Tekrarlanan kayÄ±tlarÄ± iÅŸleme\n",
    "3. AykÄ±rÄ± deÄŸer tespiti ve iÅŸlemi\n",
    "4. Ã–zellik Ã¶lÃ§ekleme ve normalleÅŸtirme\n",
    "5. Kategorik deÄŸiÅŸkenleri kodlama\n",
    "6. KatmanlÄ± eÄŸitim-test bÃ¶lmesi\n",
    "7. Ã–n iÅŸlenmiÅŸ veriyi kaydetme\n",
    "\n",
    "---\n",
    "\n",
    "## Ä°Ã§indekiler\n",
    "\n",
    "1. [Kurulum ve Veri YÃ¼kleme](#1-kurulum-ve-veri-yÃ¼kleme)\n",
    "2. [Veri Temizleme](#2-veri-temizleme)\n",
    "3. [AykÄ±rÄ± DeÄŸer Ä°ÅŸlemi](#3-aykÄ±rÄ±-deÄŸer-iÅŸlemi)\n",
    "4. [Ã–zellik MÃ¼hendisliÄŸi](#4-Ã¶zellik-mÃ¼hendisliÄŸi)\n",
    "5. [Veri DÃ¶nÃ¼ÅŸÃ¼mÃ¼](#5-veri-dÃ¶nÃ¼ÅŸÃ¼mÃ¼)\n",
    "6. [EÄŸitim-Test BÃ¶lmesi](#6-eÄŸitim-test-bÃ¶lmesi)\n",
    "7. [Ã–n Ä°ÅŸlenmiÅŸ Veriyi Kaydet](#7-Ã¶n-iÅŸlenmiÅŸ-veriyi-kaydet)\n",
    "8. [Ã–zet](#8-Ã¶zet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb891432",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Kurulum ve Veri YÃ¼kleme\n",
    "\n",
    "Gerekli kÃ¼tÃ¼phaneleri iÃ§e aktarÄ±n ve veri setini yÃ¼kleyin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c984cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LIBRARY IMPORTS\n",
    "# ============================================================\n",
    "\n",
    "# Pandas: Data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Numpy: Numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib & Seaborn: Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn: Preprocessing utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "\n",
    "# Warnings: Suppress unnecessary messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure visualization\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.precision', 2)\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")\n",
    "print(\"âœ“ Ready for data preprocessing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1099363",
   "metadata": {},
   "source": [
    "### Ham Veriyi YÃ¼kle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0e4cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw dataset\n",
    "# This is the same data we explored in the EDA phase\n",
    "data_path = '../data/raw/steel_plates_fault.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Store original shape for comparison\n",
    "original_shape = df.shape\n",
    "\n",
    "# Print initial information\n",
    "print(\"âœ“ Dataset loaded successfully!\")\n",
    "print(f\"\\nğŸ“Š Original Dataset Shape:\")\n",
    "print(f\"   - Rows: {df.shape[0]:,}\")\n",
    "print(f\"   - Columns: {df.shape[1]}\")\n",
    "\n",
    "# Separate features and target\n",
    "feature_columns = [col for col in df.columns if col != 'Class']\n",
    "target_column = 'Class'\n",
    "\n",
    "print(f\"\\nğŸ“‹ Features: {len(feature_columns)}\")\n",
    "print(f\"ğŸ¯ Target: '{target_column}'\")\n",
    "\n",
    "# Quick look at the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c773bb3",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Veri Temizleme\n",
    "\n",
    "Eksik deÄŸerleri ve tekrarlarÄ± iÅŸleyerek veri setini temizleyin.\n",
    "\n",
    "**Veri temizleme neden Ã¶nemli:**\n",
    "- Eksik deÄŸerler algoritmalarda hatalara neden olabilir\n",
    "- Tekrarlar model eÄŸitimini yanlÄ±layabilir\n",
    "- Temiz veri daha iyi model performansÄ±na yol aÃ§ar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64dd82b",
   "metadata": {},
   "source": [
    "### 2.1 Eksik DeÄŸerleri Kontrol Et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f92b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "# isnull().sum() counts missing values per column\n",
    "missing_values = df.isnull().sum()\n",
    "total_missing = missing_values.sum()\n",
    "\n",
    "print(\"ğŸ” Missing Values Check:\")\n",
    "print(f\"\\n   Total missing values: {total_missing}\")\n",
    "\n",
    "if total_missing == 0:\n",
    "    print(\"   âœ… No missing values found!\")\n",
    "    print(\"   No imputation needed.\")\n",
    "else:\n",
    "    print(\"   âš ï¸ Missing values detected!\")\n",
    "    print(\"\\n   Missing by column:\")\n",
    "    print(missing_values[missing_values > 0])\n",
    "    \n",
    "    # If there were missing values, we would handle them here:\n",
    "    # Option 1: Drop rows with missing values\n",
    "    # df = df.dropna()\n",
    "    \n",
    "    # Option 2: Fill with mean/median/mode\n",
    "    # df = df.fillna(df.mean())\n",
    "    \n",
    "    # Option 3: Fill with specific value\n",
    "    # df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beebe2d7",
   "metadata": {},
   "source": [
    "### 2.2 Tekrarlanan KayÄ±tlarÄ± KaldÄ±r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f8d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for and remove duplicate records\n",
    "# Duplicates can artificially inflate model performance\n",
    "\n",
    "# Count duplicates before removal\n",
    "num_duplicates = df.duplicated().sum()\n",
    "print(\"ğŸ” Duplicate Records Check:\")\n",
    "print(f\"\\n   Duplicates found: {num_duplicates}\")\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    # Remove duplicates, keeping the first occurrence\n",
    "    df_cleaned = df.drop_duplicates(keep='first')\n",
    "    \n",
    "    print(f\"   âœ… Removed {num_duplicates} duplicate records\")\n",
    "    print(f\"\\n   Before: {len(df):,} rows\")\n",
    "    print(f\"   After:  {len(df_cleaned):,} rows\")\n",
    "    \n",
    "    # Update the dataframe\n",
    "    df = df_cleaned\n",
    "else:\n",
    "    print(\"   âœ… No duplicates found!\")\n",
    "    print(\"   Dataset is already clean.\")\n",
    "\n",
    "# Verify the change\n",
    "print(f\"\\nğŸ“Š Current dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8da373b",
   "metadata": {},
   "source": [
    "### 2.3 Veri TÃ¼rÃ¼ DoÄŸrulamasÄ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408459fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data types are correct for each column\n",
    "# Incorrect types can cause unexpected behavior\n",
    "\n",
    "print(\"ğŸ” Data Type Validation:\\n\")\n",
    "\n",
    "# Get current data types\n",
    "dtypes = df.dtypes\n",
    "\n",
    "# Check numerical features\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"   Numerical columns: {len(numerical_cols)}\")\n",
    "\n",
    "# Check object (string) columns\n",
    "object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"   Object columns: {len(object_cols)}\")\n",
    "\n",
    "# Display data types\n",
    "print(\"\\n   Data types summary:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# Check if target is categorical\n",
    "if df['Class'].dtype == 'object':\n",
    "    print(\"\\n   âœ… Target 'Class' is categorical (object)\")\n",
    "    print(f\"   Unique classes: {df['Class'].nunique()}\")\n",
    "else:\n",
    "    print(\"\\n   âš ï¸ Target 'Class' is numerical, may need encoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f755f70e",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. AykÄ±rÄ± DeÄŸer Ä°ÅŸlemi\n",
    "\n",
    "Veri setindeki aykÄ±rÄ± deÄŸerleri belirleyin ve iÅŸleyin.\n",
    "\n",
    "**AykÄ±rÄ± deÄŸerleri iÅŸleme yÃ¶ntemleri:**\n",
    "1. **KaldÄ±r**: AÅŸÄ±rÄ± deÄŸerlere sahip satÄ±rlarÄ± sil\n",
    "2. **SÄ±nÄ±rla**: DeÄŸerleri maksimum/minimum eÅŸiÄŸe sÄ±nÄ±rla\n",
    "3. **DÃ¶nÃ¼ÅŸtÃ¼r**: Log veya diÄŸer dÃ¶nÃ¼ÅŸÃ¼mleri uygula\n",
    "4. **Koru**: AykÄ±rÄ± deÄŸerler geÃ§erliyse, koru\n",
    "\n",
    "AykÄ±rÄ± deÄŸerleri tespit etmek iÃ§in **IQR (Ã‡eyrekler ArasÄ± AralÄ±k) yÃ¶ntemi** kullanacaÄŸÄ±z."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274391a7",
   "metadata": {},
   "source": [
    "### 3.1 AykÄ±rÄ± DeÄŸer Tespiti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679a8710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using IQR method\n",
    "# IQR = Q3 - Q1\n",
    "# Outliers: values < Q1 - 1.5*IQR or > Q3 + 1.5*IQR\n",
    "\n",
    "def detect_outliers_iqr(df, column):\n",
    "    \"\"\"\n",
    "    Detect outliers in a column using IQR method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        The dataset\n",
    "    column : str\n",
    "        Column name to check\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (n_outliers, lower_bound, upper_bound, outlier_indices)\n",
    "    \"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outlier_mask = (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "    outlier_indices = df[outlier_mask].index\n",
    "    \n",
    "    return len(outlier_indices), lower_bound, upper_bound, outlier_indices\n",
    "\n",
    "# Detect outliers for all numerical features\n",
    "print(\"ğŸ” Outlier Detection (IQR Method):\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "numerical_features = [col for col in feature_columns if df[col].dtype in ['int64', 'float64']]\n",
    "outlier_summary = []\n",
    "\n",
    "for feature in numerical_features:\n",
    "    n_out, lb, ub, idx = detect_outliers_iqr(df, feature)\n",
    "    pct = (n_out / len(df)) * 100\n",
    "    \n",
    "    outlier_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Outliers': n_out,\n",
    "        'Percentage': f\"{pct:.2f}%\",\n",
    "        'Lower': f\"{lb:.2f}\",\n",
    "        'Upper': f\"{ub:.2f}\"\n",
    "    })\n",
    "\n",
    "# Display as DataFrame\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "outlier_df = outlier_df.sort_values('Outliers', ascending=False)\n",
    "print(outlier_df.head(15).to_string(index=False))\n",
    "\n",
    "# Summary\n",
    "total_outliers = outlier_df['Outliers'].sum()\n",
    "print(f\"\\nğŸ“Š Total outlier instances: {total_outliers:,}\")\n",
    "print(f\"   Features with outliers: {(outlier_df['Outliers'] > 0).sum()}/{len(numerical_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf229b",
   "metadata": {},
   "source": [
    "### 3.2 AykÄ±rÄ± DeÄŸerleri GÃ¶rselleÅŸtir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b798592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers with box plots\n",
    "# Focus on features with most outliers\n",
    "\n",
    "# Get top 6 features with most outliers\n",
    "top_outlier_features = outlier_df.head(6)['Feature'].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(top_outlier_features):\n",
    "    # Create box plot\n",
    "    bp = axes[idx].boxplot(df[feature], vert=True, patch_artist=True)\n",
    "    \n",
    "    # Color the box\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set_facecolor('lightcoral')\n",
    "    \n",
    "    # Get outlier count\n",
    "    n_out = outlier_df[outlier_df['Feature'] == feature]['Outliers'].values[0]\n",
    "    \n",
    "    axes[idx].set_title(f'{feature}\\n({n_out} outliers)', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Value', fontsize=10)\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Box Plots: Features with Most Outliers', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Red points beyond whiskers are potential outliers\")\n",
    "print(\"   Decision: Keep, cap, or remove based on domain knowledge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4740361a",
   "metadata": {},
   "source": [
    "### 3.3 AykÄ±rÄ± DeÄŸerleri Ä°ÅŸle (SÄ±nÄ±rlama YÃ¶ntemi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2514ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cap outliers using IQR boundaries\n",
    "# This preserves all data points while limiting extreme values\n",
    "\n",
    "def cap_outliers_iqr(df, column):\n",
    "    \"\"\"\n",
    "    Cap outliers to IQR boundaries.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        The dataset\n",
    "    column : str\n",
    "        Column name to cap\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Series : Column with capped values\n",
    "    \"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Cap values\n",
    "    capped = df[column].clip(lower=lower_bound, upper=upper_bound)\n",
    "    \n",
    "    return capped\n",
    "\n",
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Cap outliers for features with significant outliers (>2%)\n",
    "print(\"ğŸ”§ Capping Outliers:\\n\")\n",
    "\n",
    "capped_features = []\n",
    "for feature in numerical_features:\n",
    "    n_out, _, _, _ = detect_outliers_iqr(df_processed, feature)\n",
    "    pct = (n_out / len(df_processed)) * 100\n",
    "    \n",
    "    if pct > 2:  # Only cap if more than 2% outliers\n",
    "        df_processed[feature] = cap_outliers_iqr(df_processed, feature)\n",
    "        capped_features.append(feature)\n",
    "        print(f\"   âœ“ Capped: {feature} ({pct:.1f}% outliers)\")\n",
    "\n",
    "if len(capped_features) == 0:\n",
    "    print(\"   No features required capping (all <2% outliers)\")\n",
    "else:\n",
    "    print(f\"\\nğŸ“Š Total features capped: {len(capped_features)}\")\n",
    "\n",
    "# Verify outliers are reduced\n",
    "print(\"\\nğŸ” Verification after capping:\")\n",
    "for feature in capped_features[:3]:  # Check first 3\n",
    "    n_out_after, _, _, _ = detect_outliers_iqr(df_processed, feature)\n",
    "    print(f\"   {feature}: {n_out_after} outliers remaining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b718ad",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Ã–zellik MÃ¼hendisliÄŸi\n",
    "\n",
    "Model performansÄ±nÄ± iyileÅŸtirebilecek yeni Ã¶zellikler oluÅŸturun.\n",
    "\n",
    "**Ã‡elik hata tespiti iÃ§in Ã¶zellik mÃ¼hendisliÄŸi fikirleri:**\n",
    "1. **Oran Ã¶zellikleri**: Ä°lgili Ã¶lÃ§Ã¼mleri birleÅŸtir\n",
    "2. **ToplanmÄ±ÅŸ Ã¶zellikler**: Ã–zellik gruplarÄ± arasÄ±nda istatistikler\n",
    "3. **Alan-spesifik Ã¶zellikler**: Ã‡elik Ã¼retim bilgisine dayalÄ±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4503ed3",
   "metadata": {},
   "source": [
    "### 4.1 Yeni Ã–zellikler OluÅŸtur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c78284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new engineered features\n",
    "print(\"ğŸ”§ Feature Engineering:\\n\")\n",
    "\n",
    "# 1. Area-related features\n",
    "# Aspect ratio: Width to Height ratio\n",
    "if 'X_Maximum' in df_processed.columns and 'Y_Maximum' in df_processed.columns:\n",
    "    df_processed['Aspect_Ratio'] = (df_processed['X_Maximum'] - df_processed['X_Minimum']) / \\\n",
    "                                   (df_processed['Y_Maximum'] - df_processed['Y_Minimum'] + 1)\n",
    "    print(\"   âœ“ Created: Aspect_Ratio (width/height)\")\n",
    "\n",
    "# 2. Perimeter ratio\n",
    "if 'X_Perimeter' in df_processed.columns and 'Y_Perimeter' in df_processed.columns:\n",
    "    df_processed['Perimeter_Ratio'] = df_processed['X_Perimeter'] / (df_processed['Y_Perimeter'] + 1)\n",
    "    print(\"   âœ“ Created: Perimeter_Ratio\")\n",
    "\n",
    "# 3. Luminosity range\n",
    "if 'Maximum_of_Luminosity' in df_processed.columns and 'Minimum_of_Luminosity' in df_processed.columns:\n",
    "    df_processed['Luminosity_Range'] = df_processed['Maximum_of_Luminosity'] - df_processed['Minimum_of_Luminosity']\n",
    "    print(\"   âœ“ Created: Luminosity_Range\")\n",
    "\n",
    "# 4. Average luminosity\n",
    "if 'Sum_of_Luminosity' in df_processed.columns and 'Pixels_Areas' in df_processed.columns:\n",
    "    df_processed['Avg_Luminosity'] = df_processed['Sum_of_Luminosity'] / (df_processed['Pixels_Areas'] + 1)\n",
    "    print(\"   âœ“ Created: Avg_Luminosity\")\n",
    "\n",
    "# 5. Compactness: How compact is the fault shape\n",
    "if 'Pixels_Areas' in df_processed.columns and 'X_Perimeter' in df_processed.columns:\n",
    "    df_processed['Compactness'] = df_processed['Pixels_Areas'] / (df_processed['X_Perimeter'] ** 2 + 1)\n",
    "    print(\"   âœ“ Created: Compactness\")\n",
    "\n",
    "# Update feature columns list\n",
    "new_features = ['Aspect_Ratio', 'Perimeter_Ratio', 'Luminosity_Range', 'Avg_Luminosity', 'Compactness']\n",
    "new_features = [f for f in new_features if f in df_processed.columns]\n",
    "\n",
    "print(f\"\\nğŸ“Š New features created: {len(new_features)}\")\n",
    "print(f\"   Total features now: {df_processed.shape[1] - 1}\")  # -1 for target\n",
    "\n",
    "# Display new features\n",
    "print(\"\\nğŸ“‹ New Feature Statistics:\")\n",
    "df_processed[new_features].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068c92c2",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Veri DÃ¶nÃ¼ÅŸÃ¼mÃ¼\n",
    "\n",
    "Makine Ã¶ÄŸrenimi algoritmalarÄ± iÃ§in Ã¶zellikleri dÃ¶nÃ¼ÅŸtÃ¼rÃ¼n.\n",
    "\n",
    "**DÃ¶nÃ¼ÅŸÃ¼m neden Ã¶nemli:**\n",
    "- BirÃ§ok algoritma normal daÄŸÄ±lÄ±mlÄ± veri varsayar\n",
    "- Ã–zellikler benzer Ã¶lÃ§eklerde olmalÄ±\n",
    "- Kategorik deÄŸiÅŸkenler kodlama gerektirir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f70ab",
   "metadata": {},
   "source": [
    "### 5.1 Hedef DeÄŸiÅŸkeni Kodla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d689392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable (Class)\n",
    "# LabelEncoder converts categorical labels to integers\n",
    "\n",
    "print(\"ğŸ”§ Encoding Target Variable:\\n\")\n",
    "\n",
    "# Create label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the target\n",
    "df_processed['Class_Encoded'] = label_encoder.fit_transform(df_processed['Class'])\n",
    "\n",
    "# Display the mapping\n",
    "print(\"   Label Mapping:\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    count = (df_processed['Class_Encoded'] == i).sum()\n",
    "    print(f\"   {class_name:15} â†’ {i} ({count} samples)\")\n",
    "\n",
    "print(f\"\\n   âœ… Target encoded successfully!\")\n",
    "print(f\"   Classes: {len(label_encoder.classes_)}\")\n",
    "\n",
    "# Store the encoder for later use (inverse transform)\n",
    "# This is important for interpreting predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c2f7d3",
   "metadata": {},
   "source": [
    "### 5.2 Ã–zellik Ã–lÃ§ekleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5bb6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for scaling\n",
    "# Get all numerical features (excluding target)\n",
    "feature_cols = [col for col in df_processed.columns \n",
    "                if col not in ['Class', 'Class_Encoded']]\n",
    "\n",
    "print(\"ğŸ”§ Feature Scaling Preparation:\\n\")\n",
    "print(f\"   Features to scale: {len(feature_cols)}\")\n",
    "\n",
    "# Display statistics before scaling\n",
    "print(\"\\nğŸ“Š Feature Statistics BEFORE Scaling:\")\n",
    "print(df_processed[feature_cols].describe().round(2).T[['mean', 'std', 'min', 'max']].head(10))\n",
    "\n",
    "print(\"\\nğŸ’¡ Note: Features have very different scales!\")\n",
    "print(\"   This can cause problems for algorithms like SVM, KNN, Neural Networks\")\n",
    "print(\"   We'll apply StandardScaler to normalize features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0e331c",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. EÄŸitim-Test BÃ¶lmesi\n",
    "\n",
    "Veriyi eÄŸitim ve test setlerine bÃ¶lÃ¼n.\n",
    "\n",
    "**Ã–nemli hususlar:**\n",
    "- **KatmanlÄ±**: Her iki sette de sÄ±nÄ±f oranlarÄ±nÄ± koru\n",
    "- **Rastgele durum**: Tekrarlanabilirlik iÃ§in\n",
    "- **Test boyutu**: Test iÃ§in genellikle %20-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135f0c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrix (X) and target vector (y)\n",
    "X = df_processed[feature_cols].values\n",
    "y = df_processed['Class_Encoded'].values\n",
    "\n",
    "print(\"ğŸ“Š Data Preparation for Split:\\n\")\n",
    "print(f\"   Feature matrix shape: {X.shape}\")\n",
    "print(f\"   Target vector shape: {y.shape}\")\n",
    "print(f\"   Number of features: {X.shape[1]}\")\n",
    "print(f\"   Number of classes: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abe4dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "# - test_size=0.2: 20% for testing, 80% for training\n",
    "# - random_state=42: Ensures reproducibility\n",
    "# - stratify=y: Maintains class proportions\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  # Important for imbalanced datasets!\n",
    ")\n",
    "\n",
    "print(\"âœ… Train-Test Split Complete!\\n\")\n",
    "print(f\"   Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"   Test set:     {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.0f}%)\")\n",
    "\n",
    "# Verify stratification\n",
    "print(\"\\nğŸ“Š Class Distribution Verification:\")\n",
    "print(\"\\n   Training set:\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    train_count = (y_train == i).sum()\n",
    "    train_pct = train_count / len(y_train) * 100\n",
    "    print(f\"      {class_name}: {train_count} ({train_pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n   Test set:\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    test_count = (y_test == i).sum()\n",
    "    test_pct = test_count / len(y_test) * 100\n",
    "    print(f\"      {class_name}: {test_count} ({test_pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n   âœ… Stratification verified - proportions are similar!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5290ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler to the features\n",
    "# IMPORTANT: Fit only on training data, transform both train and test\n",
    "\n",
    "print(\"ğŸ”§ Applying StandardScaler:\\n\")\n",
    "\n",
    "# Create scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data only (to prevent data leakage)\n",
    "# Then transform training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform test data using the same scaler (fitted on training)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"   âœ… Scaling applied successfully!\")\n",
    "print(\"\\nğŸ“Š Statistics AFTER Scaling (Training Set):\")\n",
    "print(f\"   Mean: {X_train_scaled.mean():.6f} (should be ~0)\")\n",
    "print(f\"   Std:  {X_train_scaled.std():.6f} (should be ~1)\")\n",
    "print(f\"   Min:  {X_train_scaled.min():.2f}\")\n",
    "print(f\"   Max:  {X_train_scaled.max():.2f}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Why fit only on training data?\")\n",
    "print(\"   - Prevents 'data leakage' from test set\")\n",
    "print(\"   - Test set should simulate unseen data\")\n",
    "print(\"   - Scaling parameters come only from training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66e1fe1",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Ã–n Ä°ÅŸlenmiÅŸ Veriyi Kaydet\n",
    "\n",
    "Model eÄŸitiminde kullanmak iÃ§in Ã¶n iÅŸlenmiÅŸ veriyi kaydedin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc264be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create processed data directory if it doesn't exist\n",
    "import os\n",
    "processed_dir = '../data/processed'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# Save the preprocessed dataframe\n",
    "df_processed.to_csv(f'{processed_dir}/steel_plates_preprocessed.csv', index=False)\n",
    "print(f\"âœ… Saved: {processed_dir}/steel_plates_preprocessed.csv\")\n",
    "\n",
    "# Save train/test splits as numpy arrays\n",
    "np.save(f'{processed_dir}/X_train.npy', X_train_scaled)\n",
    "np.save(f'{processed_dir}/X_test.npy', X_test_scaled)\n",
    "np.save(f'{processed_dir}/y_train.npy', y_train)\n",
    "np.save(f'{processed_dir}/y_test.npy', y_test)\n",
    "print(f\"âœ… Saved: X_train.npy, X_test.npy, y_train.npy, y_test.npy\")\n",
    "\n",
    "# Save the feature names\n",
    "with open(f'{processed_dir}/feature_names.txt', 'w') as f:\n",
    "    for col in feature_cols:\n",
    "        f.write(col + '\\n')\n",
    "print(f\"âœ… Saved: feature_names.txt\")\n",
    "\n",
    "# Save the label encoder classes\n",
    "with open(f'{processed_dir}/class_names.txt', 'w') as f:\n",
    "    for cls in label_encoder.classes_:\n",
    "        f.write(cls + '\\n')\n",
    "print(f\"âœ… Saved: class_names.txt\")\n",
    "\n",
    "print(\"\\nğŸ“ All preprocessed data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78546617",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Ã–zet\n",
    "\n",
    "### ğŸ“Š Ã–n Ä°ÅŸleme Ã–zeti\n",
    "\n",
    "| AdÄ±m | Ã–nce | Sonra | Ä°ÅŸlem |\n",
    "|------|------|-------|-------|\n",
    "| **Ã–rnekler** | 1.941 | {gÃ¼ncellendi} | Temizlendi |\n",
    "| **Ã–zellikler** | 27 | {gÃ¼ncellendi} | + MÃ¼hendislik |\n",
    "| **Eksik** | 0 | 0 | Gerekmedi |\n",
    "| **Tekrarlar** | Kontrol | KaldÄ±rÄ±ldÄ± | Temizlendi |\n",
    "| **AykÄ±rÄ±lar** | Tespit | SÄ±nÄ±rlandÄ± | IQR yÃ¶ntemi |\n",
    "| **Ã–lÃ§ekleme** | Ham | Standardize | StandardScaler |\n",
    "\n",
    "### ğŸ“ OluÅŸturulan Dosyalar\n",
    "\n",
    "```\n",
    "data/processed/\n",
    "â”œâ”€â”€ steel_plates_preprocessed.csv  # Tam Ã¶n iÅŸlenmiÅŸ veri seti\n",
    "â”œâ”€â”€ X_train.npy                    # EÄŸitim Ã¶zellikleri (Ã¶lÃ§ekli)\n",
    "â”œâ”€â”€ X_test.npy                     # Test Ã¶zellikleri (Ã¶lÃ§ekli)\n",
    "â”œâ”€â”€ y_train.npy                    # EÄŸitim etiketleri\n",
    "â”œâ”€â”€ y_test.npy                     # Test etiketleri\n",
    "â”œâ”€â”€ feature_names.txt              # Ã–zellik sÃ¼tun adlarÄ±\n",
    "â””â”€â”€ class_names.txt                # SÄ±nÄ±f etiket adlarÄ±\n",
    "```\n",
    "\n",
    "### ğŸš€ Sonraki AdÄ±mlar\n",
    "\n",
    "1. **Model EÄŸitimi**: Modelleri eÄŸitmek iÃ§in Ã¶n iÅŸlenmiÅŸ veriyi kullan\n",
    "2. **Model KarÅŸÄ±laÅŸtÄ±rmasÄ±**: Birden fazla algoritmayÄ± deÄŸerlendir\n",
    "3. **Hiperparametre Ayarlama**: Model parametrelerini optimize et\n",
    "4. **Model DeÄŸerlendirmesi**: AyrÄ±lmÄ±ÅŸ test setinde test et\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **AÅŸama 2 (Ã–n Ä°ÅŸleme) TamamlandÄ±!**\n",
    "\n",
    "Veri artÄ±k makine Ã¶ÄŸrenimi iÃ§in hazÄ±r! ğŸ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89280053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary Statistics\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ“Š FINAL PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nâœ… Original samples:     {original_shape[0]:,}\")\n",
    "print(f\"âœ… Processed samples:    {len(df_processed):,}\")\n",
    "print(f\"âœ… Original features:    {original_shape[1] - 1}\")\n",
    "print(f\"âœ… Final features:       {len(feature_cols)}\")\n",
    "print(f\"âœ… Training samples:     {X_train_scaled.shape[0]:,}\")\n",
    "print(f\"âœ… Test samples:         {X_test_scaled.shape[0]:,}\")\n",
    "print(f\"âœ… Classes:              {len(label_encoder.classes_)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Ready for Model Training! ğŸš€\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
